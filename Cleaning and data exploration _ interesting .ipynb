{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction:\nThis project had launched by <font color=blue> ASHRAE Organization </font> in order to :  \ndevelop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data that we will use, comes from over 1,000 buildings over a three-year timeframe."},{"metadata":{},"cell_type":"markdown","source":"# Importing datas :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing datas of  training building energy consumption \ndt_tr=pd.read_csv('/kaggle/input/ashrae-energy-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing datas of building caracteristic. \ndt_bd=pd.read_csv('/kaggle/input/ashrae-energy-prediction/building_metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing training datas of weather conditions .\ndt_tw=pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing datas of test building that we should their energy consumption.\ndt_ts=pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing test datas of weather conditions .\ndt_tsw=pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values for training building consumptions:\nbuilding_miss=pd.DataFrame({c:[sum(dt_tr[c].isna()),(sum(dt_tr[c].isna())/len(dt_tr[c]))*100] \\\n                            for c in dt_tr.columns} ,index=['Total','%'])\nbuilding_miss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values for test building consumptions:\nbuilding_test_miss=pd.DataFrame({c:[sum(dt_ts[c].isna()),(sum(dt_ts[c].isna())/len(dt_ts[c]))*100] \\\n                            for c in dt_ts.columns} ,index=['Total','%'])\nbuilding_test_miss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values building caracteristic:\nbuild_miss=pd.DataFrame({c:[sum(dt_bd[c].isna()),(sum(dt_bd[c].isna())/len(dt_bd[c]))*100] \\\n                            for c in dt_bd.columns} ,index=['Total','%'])\nbuild_miss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous table, which summarize the missing values of building caracteristic , show that this dataframe has two\nvariables , that have more than 50% of missing values . So, we decide to delete theses variables , which contains\nvery big quanity of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"del (dt_bd['year_built'])\ndel (dt_bd['floor_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now examinate the missing values of the weather conditions .\nweath_tr_miss=pd.DataFrame({c:[sum(dt_tw[c].isna()),(sum(dt_tw[c].isna())/len(dt_tw))*100] \\\n                            for c in dt_tw.columns},index=['total','%'])\nweath_tr_miss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test weather conditions.\nweath_ts_miss=pd.DataFrame({c:[sum(dt_tsw[c].isna()),(sum(dt_tsw[c].isna())/len(dt_tsw))*100] \\\n                            for c in dt_tsw.columns},index=['total','%'])\nweath_ts_miss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice from the above table , which summarize the weather variables missing values, that the two variables cloud_coverage and precip_depth_1_hr have more than 30% of missing values . So we decide to drop\nthseses variables because of an extrapolation of theses big quantity of missing values could inevatilly affect our predictions ."},{"metadata":{"trusted":true},"cell_type":"code","source":"del (dt_tw['cloud_coverage'])\ndel(dt_tw['precip_depth_1_hr'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(dt_tsw['cloud_coverage'])\ndel(dt_tsw['precip_depth_1_hr'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear regression , which link air_temperature to dew_temperature for both training and tests datas\nfrom sklearn.linear_model import LinearRegression\nmodel_tr=LinearRegression()\nmodel_ts=LinearRegression()\nindtr=dt_tw[(dt_tw['air_temperature'].notna()==True) & (dt_tw['dew_temperature'].notna()==True)].index\nindts=dt_tsw[(dt_tsw['air_temperature'].notna()==True) & (dt_tsw['dew_temperature'].notna()==True)].index\nmodel_tr.fit(pd.DataFrame({'cte':np.ones(len(indtr)),'air':dt_tw.iloc[indtr]['air_temperature']}),\\\n             dt_tw.iloc[indtr]['dew_temperature'])\nmodel_ts.fit(pd.DataFrame({'cte':np.ones(len(indts)),'air':dt_tsw.iloc[indts]['air_temperature']}),\\\n             dt_tsw.iloc[indts]['dew_temperature'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intertr=model_tr.intercept_   # the intercept of the linear regression for the training datas .\ncoeftr=model_tr.coef_         # the coefficient of the linear regression for the training datas .\ninterts=model_ts.intercept_   # the intercept of the linear regression for the test datas.\ncoefts=model_ts.coef_         # the coefficient of the linear regression for the test datas .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n# Let's visualize theses linear regreesion in compare with the true relationship between the above two variables.\nfig=plt.figure(figsize=(20,20))\nfig.add_subplot(1,2,1)\nplt.plot(dt_tw.iloc[indtr]['air_temperature'],dt_tw.iloc[indtr]['dew_temperature'],\\\n         label='true train dew_temperature')\nplt.plot(dt_tw.iloc[indtr]['air_temperature'],intertr + coeftr[1] * dt_tw.iloc[indtr]['air_temperature'],\\\n        label='regressed train dew_temperature',color='black',linewidth=2,linestyle='dashed')\nplt.legend()\nplt.xlabel('air_temperature')\nplt.ylabel('dew_temperature')\nplt.title('Regressed dew_temperature VS true dew_temperature')\nfig.add_subplot(1,2,2)\nplt.plot(dt_tsw.iloc[indts]['air_temperature'],dt_tsw.iloc[indts]['dew_temperature'],\\\n         label='true test dew_temperature',color='yellow')\nplt.plot(dt_tsw.iloc[indts]['air_temperature'],interts + coefts[1] * dt_tsw.iloc[indts]['air_temperature'],\\\n        label='regressed test dew_temperature',color='green',linewidth=2,linestyle='dashed')\nplt.legend()\nplt.xlabel('air_temperature')\nplt.ylabel('dew_temperature')\nplt.title('Regressed dew_temperature VS true dew_temperature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the number of dew_temperature and air_temperature that could be imputed by linear relation\n# between theses two variables .\nl=len(dt_tw[((dt_tw['dew_temperature'].isna()==False) & (dt_tw['air_temperature'].isna()==True))|\\\n     ((dt_tw['dew_temperature'].isna()==True) & (dt_tw['air_temperature'].isna()==False))])\nprint('There is {} missing values in dt_tw , which could imputed by the linear relation between dew_temperature and air_temperature'.format(l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the number of dew_temperature and air_temperature that could be imputed by linear relation\n# between theses two variables in dt_tsw.\nl=len(dt_tsw[((dt_tsw['dew_temperature'].isna()==False) & (dt_tsw['air_temperature'].isna()==True))|\\\n     ((dt_tsw['dew_temperature'].isna()==True) & (dt_tsw['air_temperature'].isna()==False))])\nprint('There is {} missing values in dt_tsw , which could imputed by the linear \\\nrelation between dew_temperature and air_temperature'.format(l))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now use the linear regression to impute air_temperature and dew_temperature missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute train missing values .\nmis_tw=dt_tw[((dt_tw['dew_temperature'].isna()==False) & (dt_tw['air_temperature'].isna()==True))|\\\n     ((dt_tw['dew_temperature'].isna()==True) & (dt_tw['air_temperature'].isna()==False))]\nind=mis_tw.index\nfor i in ind :\n    if pd.isnull(dt_tw.at[i,'dew_temperature']):\n        dt_tw.at[i,'dew_temperature']=model_tr.predict(np.array([1,dt_tw.at[i,'air_temperature']]).\\\n                                                       reshape(1,-1))\n    else:\n        dt_tw.iloc.at[i,'air_temperature']=(dt_tw.at[i,'dew_temperature']-intertr)/coeftr[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute test missing values .\nmis_tsw=dt_tsw[((dt_tsw['dew_temperature'].isna()==False) & (dt_tsw['air_temperature'].isna()==True))|\\\n     ((dt_tsw['dew_temperature'].isna()==True) & (dt_tsw['air_temperature'].isna()==False))]\nindts=mis_tsw.index\nfor i in indts :\n    if pd.isnull(dt_tsw.at[i,'dew_temperature']):\n        dt_tsw.at [i,'dew_temperature']=model_tr.predict(np.array([1,dt_tsw.at[i,'air_temperature']]).\\\n                                                       reshape(1,-1))\n    else:\n        dt_tsw.at [i,'air_temperature']=(dt_tsw.at[i,'dew_temperature']-intertr)/coeftr[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank to high colinearity between the variables air_temperature and dew_temperature. We will save only the variable air_temperature , which known to have more influence on the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"del(dt_tw['dew_temperature'])\ndel(dt_tsw['dew_temperature'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For the rest of missing values , we will try to find the near values of the missing one. Indeed, the nearst wich will be defined in function of site_id and the other time parameters. For this reason we will use the KnearstNeighbors algorhitm to imput theses missing values ."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heureunder , we will define new variables which can allow to decorticate times parameters.\ndt_tw['month']=pd.to_datetime(dt_tw['timestamp']).dt.month\ndt_tw['day']=pd.to_datetime(dt_tw['timestamp']).dt.day\ndt_tw['hour']=pd.to_datetime(dt_tw['timestamp']).dt.hour\ndt_tw['day_week']=pd.to_datetime(dt_tw['timestamp']).dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will do same thing for test weather .\ndt_tsw['month']=pd.to_datetime(dt_tsw['timestamp']).dt.month\ndt_tsw['day']=pd.to_datetime(dt_tsw['timestamp']).dt.day\ndt_tsw['hour']=pd.to_datetime(dt_tsw['timestamp']).dt.hour\ndt_tsw['day_week']=pd.to_datetime(dt_tsw['timestamp']).dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_values(vr):\n    \"\"\"This function allow to determinate the training data , which could be used to train further \n    alghorithm in order to predict the target value of the variable introduced as parameter.\n    \n    Args:\n    \n    vr(string): The name of the target variable, that we want to predict .\n    \n    Returns:\n    \n    X,Y : The train data extracted from dt_tw and dt_tsw , to serve in training an machine learning alghorithm.\n    \"\"\"\n    id_tr=dt_tw[dt_tw[vr].notna()==True].index\n    id_ts=dt_tsw[dt_tsw[vr].notna()==True].index\n    X=dt_tw.iloc[id_tr][['site_id','month','day','hour']]\n    X=X.append(dt_tsw.iloc[id_ts][['site_id','month','day','hour']])\n    Y=dt_tw[vr].iloc[id_tr]\n    Y=Y.append(dt_tsw[vr].iloc[id_ts])\n    return X,Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imputer(var,algo,df1,df2):\n    \"\"\" This function , allow to impute the missing values of the variable namely var, which introduced as\n        parameter in the dataframes df1 and df2 .This on base of the trained machine learning algo, which \n        had introduced as parameter.\n        \n        Args:\n        \n        vr(string): the name of the variable , which we will impute his missing values .\n        algo : the trained machine learning , which will predict the variable missing values .\n        df1,df2(DataFrame): the dataframe, that we will impute on it the missing values .\n        \n        \"\"\"\n    ii_tr=df1[df1[var].isna()==True].index\n    ii_ts=df2[df2[var].isna()==True].index\n    for i in ii_tr :\n        df1.at[i,var]=algo.predict(df1.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\n    for j in ii_ts :\n        df2.at[j,var]=algo.predict(df2.iloc[j][['site_id','month','day','hour']].to_numpy().reshape(1,-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### air_temperature missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nX,Y=training_values('air_temperature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer_temperature=KNeighborsRegressor()\nimputer_temperature.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute air_temperature missing values .\nimputer('air_temperature',imputer_temperature,dt_tw,dt_tsw)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### sea_level_pressure missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"X1,Y1=training_values('sea_level_pressure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer_pressure=KNeighborsRegressor()\nimputer_pressure.fit(X1,Y1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute sea_level_pressure missing values .\nimputer('sea_level_pressure',imputer_pressure,dt_tw,dt_tsw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Wind direction missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"X2,Y2=training_values('wind_direction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer_wind_direction=KNeighborsRegressor()\nimputer_wind_direction.fit(X2,Y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute wind_direction missing values .\nimputer('wind_direction',imputer_wind_direction,dt_tw,dt_tsw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### wind speed missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"X3,Y3=training_values('wind_speed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer_wind_speed=KNeighborsRegressor()\nimputer_wind_speed.fit(X3,Y3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute wind_speed missing values .\nimputer('wind_direction',imputer_wind_speed,dt_tw,dt_tsw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"medianprops={'color':\"black\"}\nmeanprops={'marker':'o','markeredgecolor':'black','markerfacecolor':'firebrick'}\nplt.boxplot(dt_tr['meter_reading'],labels=['meter_reading'],vert=False ,medianprops=medianprops,\\\n           meanprops=meanprops,showmeans=True,showfliers=False,patch_artist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice from the boxplot above , that there is meter_reading with zero values . Theses values , seem to be outliers values . So we will drop all rows wich contains meter_reading with zero value ."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_tr=dt_tr[dt_tr['meter_reading']!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare datas :"},{"metadata":{},"cell_type":"markdown","source":"<font style=Italy> <font color=green>Hereunder , we will make jointure between  dt_tr ,dt_bd, dt_tw , in order to bring together all training datas in the same dataframe table . Besides , we do the same work for test datas .</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we will do an left external Juncture between dt_tr,dt_bd, \n# which traduct an association of the buiding consumption datas \n# with the datas of building carateristic .\ntr1=pd.merge(dt_tr,dt_bd,how='left',on='building_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now , we will do an left external juncture between the last \n# obtained datafram tr1 , with dt_tw, which will allow to \n# associate the building consumption with the weather conditions.\ntr=pd.merge(tr1,dt_tw,on=['site_id','timestamp'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr[tr['site_id']==0]['meter_reading']=[0.2931* c for c in (tr[tr['site_id']==0]['meter_reading'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will the same work for the test datas.\nts1=pd.merge(dt_ts,dt_bd,on='building_id',how='left')\nts=pd.merge(ts1,dt_tsw,on=['site_id','timestamp'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Function to reduce the DF size'''\n# source: https://www.kaggle.com/kernels/scriptcontent/3684066/download\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_tr=reduce_mem_usage(dt_tr)\ndt_bd=reduce_mem_usage(dt_bd)\ndt_tw=reduce_mem_usage(dt_tw)\ndt_ts=reduce_mem_usage(dt_ts)\ndt_tsw=reduce_mem_usage(dt_tsw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will delete all useles dataframes to reduce memory :\ndel(dt_tr)\ndel(dt_bd)\ndel(dt_tw)\ndel(dt_ts)\ndel(dt_tsw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts1=reduce_mem_usage(ts1)\ntr1=reduce_mem_usage(tr1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n# time missing values\nmtr=tr[(tr['month'].isna()==True)|(tr['day'].isna()==True)|(tr['hour'].isna()==True)|\\\n      (tr['day_week'].isna()==True)].index # Note the same index for all variables missing values \nmts=ts[(ts['month'].isna()==True)|(ts['day'].isna()==True)|(ts['hour'].isna()==True)|\\\n      (ts['day_week'].isna()==True)].index\nfor i in mtr :\n    tr.at[i,'month']=datetime.strptime(tr.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').month\n    tr.at[i,'day']=datetime.strptime(tr.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').day\n    tr.at[i,'hour']=datetime.strptime(tr.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').hour\n    tr.at[i,'day_week']=datetime.strptime(tr.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').weekday()\nfor i in mts :\n    ts.at[i,'month']=datetime.strptime(ts.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').month\n    ts.at[i,'day']=datetime.strptime(ts.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').day\n    ts.at[i,'hour']=datetime.strptime(ts.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').hour\n    ts.at[i,'day_week']=datetime.strptime(ts.at[i,'timestamp'],'%Y-%m-%d %H:%M:%S').weekday()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the above missing values predictor to impute the new missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute air_temperature missing values .\ntrt=tr[tr['air_temperature'].isna()==True].index\ntst=ts[ts['air_temperature'].isna()==True].index\nfor i in trt :\n    tr.at[i,'air_temperature']=imputer_temperature.predict(tr.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))    \nfor i in tst :\n    ts.at[i,'air_temperature']=imputer_temperature.predict(ts.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute sea_level_pressure missing values .\nprt=tr[tr['sea_level_pressure'].isna()==True].index\npst=ts[ts['sea_level_pressure'].isna()==True].index\nfor i in prt :\n    tr.at[i,'sea_level_pressure']=imputer_pressure.predict(tr.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\nfor i in pst :\n    ts.at[i,'sea_level_pressure']=imputer_pressure.predict(ts.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute wind_direction missing values.\ndrt=tr[tr['wind_direction'].isna()==True].index\ndst=ts[ts['wind_direction'].isna()==True].index\nfor i in drt :\n    tr.at[i,'wind_direction']=imputer_wind_direction.predict(tr.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\nfor i in dst :\n    ts.at[i,'wind_direction']=imputer_wind_direction.predict(ts.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute wind_speed missing values .\nsrt=tr[tr['wind_speed'].isna()==True].index\nsts=ts[ts['wind_speed'].isna()==True].index\nfor i in srt :\n    tr.at[i,'wind_speed']=imputer_wind_speed.predict(tr.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))\nfor i in sts :\n    ts.at[i,'wind_speed']=imputer_wind_speed.predict(ts.iloc[i][['site_id','month','day','hour']].to_numpy().reshape(1,-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features engineering: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let 's visualize the meter_reding in function of month\nfreq=tr['month'].value_counts()\nmois=list(freq.index)\nmois_consumption=np.array([tr[tr['month']==c]['meter_reading'].sum()/freq.loc[c] for c in mois])\nind=np.argsort(mois_consumption)\nmonth=[]\nmonth_consumption=[]\nfor i in ind :\n    month+=[str(mois[i])]\n    month_consumption+=[mois_consumption[i]]\nplt.bar(month,month_consumption)\nplt.xlabel('Month',fontsize=12,color='red')\nplt.ylabel('Mean energy consumption',fontsize=12,color='red')\nplt.title('Mean energy consumption per month',color='orange',fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodage_month={7:1,8:2,10:3,12:4,9:5,11:6,1:7,2:8,6:9,3:10,5:11,4:12}\ntr['month']=[encodage_month[c] for c in tr['month']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let 's visualize the meter_reding in function of day of week\nfreq=tr['day_week'].value_counts()\njours=list(freq.index)\njours_consumption=np.array([tr[tr['day_week']==c]['meter_reading'].sum()/freq.loc[c] for c in jours])\nind=np.argsort(jours_consumption)\ndays=[]\ndays_consumption=[]\nfor i in ind :\n    days+=[str(jours[i])]\n    days_consumption+=[jours_consumption[i]]\nplt.bar(days,days_consumption)\nplt.xlabel('Day of week ',fontsize=12,color='red')\nplt.ylabel('Mean energy consumption',fontsize=12,color='red')\nplt.title('Mean energy consumption per day of week',color='orange',fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr['month'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodage_days_week={6:1,5:2,0:3,4:4,3:5,1:6,2:7}\ntr['day_week']=[encodage_days_week[c] for c in tr['day_week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let 's visualize the meter_reding in function of day hours\nfig=plt.figure(figsize=(10,8))\nfreq=tr['hour'].value_counts()\nheures=list(freq.index)\nheures_consumption=np.array([tr[tr['hour']==c]['meter_reading'].sum()/freq.loc[c] for c in heures])\nind=np.argsort(heures_consumption)\nhours=[]\nhours_consumption=[]\nfor i in ind :\n    hours+=[str(heures[i])]\n    hours_consumption+=[heures_consumption[i]]\nplt.barh(hours,hours_consumption)\nplt.ylabel('Hours',fontsize=12,color='red')\nplt.xlabel('Mean energy consumption',fontsize=12,color='red')\nplt.title('Mean energy consumption per hour',color='orange',fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodage_hours={6:1,5:2,3:3,4:4,7:5,8:6,2:7,1:8,0:9,9:10,10:11,23:12,11:13,12:14,22:15,13:16,14:17,17:18,21:19,\\\n               18:20,20:21,19:22,16:23,15:24}\ntr['hour']=[encodage_hours[c] for c in tr['hour']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let 's visualize the meter_reding in function of day of month \nfig=plt.figure(figsize=(10,12))\nfreq=tr['day'].value_counts()\njmois=list(freq.index)\njmois_consumption=np.array([tr[tr['day']==c]['meter_reading'].sum()/freq.loc[c] for c in jmois])\nind=np.argsort(jmois_consumption)\ndmonth=[]\ndmonth_consumption=[]\nfor i in ind :\n    dmonth+=[str(jmois[i])]\n    dmonth_consumption+=[jmois_consumption[i]]\nplt.barh(dmonth,dmonth_consumption)\nplt.ylabel('Day of month',fontsize=12,color='red')\nplt.xlabel('Mean energy consumption',fontsize=12,color='red')\nplt.title('Mean energy consumption per day of month',color='orange',fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodage_day_month={17:1,18:2,4:3,2:4,19:5,20:6,5:7,21:8,1:9,22:10,30:11,23:12,3:13,26:14,25:15,24:16,31:17,6:18,\\\n                   7:19,29:20,28:21,27:22,16:23,11:24,15:25,12:26,14:27,8:28,13:29,10:30,9:31}\ntr['day']=[encodage_day_month[c] for c in tr['day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(tr['timestamp']) # useless variable\ntr=reduce_mem_usage(tr) # reduce memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts['month']=[encodage_month[c] for c in ts['month']]\nts['day']=[encodage_day_month[c] for c in ts['day']]\nts['hour']=[encodage_hours[c] for c in ts['hour']]\nts['day_week']=[encodage_days_week[c] for c in ts['day_week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(ts['timestamp']) # useless variable\nts=reduce_mem_usage(ts) # reduce memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hereunder , we will visualize the mean hour consumption energy per primary use.\nfig=plt.figure(figsize=(15,10))\ntr_user=list(tr['primary_use'].unique())\nfrequency_use=tr['primary_use'].value_counts()\nuser_consumption=np.array([((((tr[tr['primary_use']==c])['meter_reading']).sum())/frequency_use.loc[c])\\\n                  for c in tr_user ])\nind=np.argsort(user_consumption)\nuser=[]\nconsumption=[]\nfor i in ind :\n    user +=[tr_user[i]]\n    consumption+=[user_consumption[i]]\nplt.barh(user,consumption,color='red')\nplt.ylabel('primary_use',fontsize=18)\nplt.xlabel('meter_reading',fontsize=18)\nplt.title('consumption energy per primary_use for training datas',color='blue',fontsize=18)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart above , could help us to give a meaningful encoding of the primary_use variable , properly  to the mean consumption of each type of primary use building . So we will use the following encoding of this variable, wich allow to give weight in concordance with each type primary consumption.  \n\nPrimary_use | Weights\n----------- |--------\nReligious worship | 1\nWarehouse/storage | 2\nOther| 3\nTechnology/science | 4\nRetail            | 5\nParking           | 6\nManufacturing industrial | 7\nPublic services | 8\nLodging/residential      | 9\nFood sales and services  | 10\nUtility | 11\nEntertainement /public assembly | 12\nOffice  | 13\nHealthcare | 14\nServices | 15\nEducation | 16"},{"metadata":{"trusted":true},"cell_type":"code","source":"encodage={'Religious worship':1,'Warehouse/storage':2,'Technology/science':4,'Other':3,'Retail':5,'Parking':6,\\\n         'Lodging/residential':9,'Manufacturing/industrial':7,'Public services':8,'Food sales and service':10,\\\n         'Entertainment/public assembly':12,'Utility':11,'Office':13,'Healthcare':14,'Services':15,\\\n         'Education':16}\ntr['primary_use']=[encodage[c] for c in tr['primary_use']]\nts['primary_use']=[encodage[c] for c in ts['primary_use']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hereunder , we will visualize the mean hour consumption energy per type of consumption .\nfig=plt.figure(figsize=(15,10))\ntr_type=[0,1,2,3]\ntype_cor={0:'electricity',1:'chilledwater',2:'steam',3:'hotwater'}\ntype_freq_use=tr['meter'].value_counts()\ntype_consumption=np.array([(tr[tr['meter']==c]['meter_reading'].sum())/(type_freq_use[c]) for c in tr_type])\nidn=np.argsort(type_consumption)\ncs=[]\ntyp=[]\nfor i in idn :\n    cs+=[type_consumption[i]]\n    typ+=[type_cor[tr_type[i]]]\nplt.bar(typ,cs,color='orange')\nplt.xlabel('Energy type',fontsize=18)\nplt.ylabel('Energy consumption',fontsize=18)\nplt.title('Energy consumption per energy type',fontsize=18,color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart above , will help us to reencode the variable titled meter , in concordance with the mean energy consumption of each energy type.  \n\nType Â | Weight \n----- | -------\nelectricity | 1\nhotwater | 2\nchilledwater | 3\nsteam | 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"meter_encodage={0:1,3:2,1:3,2:4}\ntr['meter']=[meter_encodage[c] for c in tr['meter']]\nts['meter']=[meter_encodage[c] for c in ts['meter']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse multivariate :"},{"metadata":{},"cell_type":"markdown","source":"### Quantitative variables :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We analyse the correlation between the quantitative variables .\ncor=(tr[['meter_reading','square_feet','air_temperature','sea_level_pressure',\\\n      'wind_direction','wind_speed']]).corr()\ncor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline performance"},{"metadata":{},"cell_type":"markdown","source":"## Linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will test a Ridge linear regression , so will do the following steps :\n* We will use Gridsearchcv to tunning the best hyperparameter.\n* split the data into train and test datas .\n* perform the baseline performance of the ridge regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the datas .\ntrain =tr.drop(columns=['meter_reading','building_id','site_id'])\ntest=ts.drop(columns=['building_id','site_id'])\ntarget=tr['meter_reading']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtr,Xts,Ytr,Yts=train_test_split(train,target,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning the best hyperparametrs\nlinear=Ridge()\nparametrs={'alpha':np.logspace(-3,3,10)}\nreg_lin=GridSearchCV(linear,parametrs,cv=5)\nreg_lin.fit(Xtr,Ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc1=reg_lin.score(Xts,Yts)\nprint('The baseline score is : {}'.format(sc1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test.drop(columns='row_id')\nsamples=pd.DataFrame({'row_id':ts['row_id']})\nsamples['meter_reading']=reg_lin.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples.to_csv('ridge.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.to_csv('tr.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts.to_csv('ts.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}